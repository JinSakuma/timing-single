{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsakuma/anaconda3/lib/python3.7/site-packages/torchaudio/backend/utils.py:54: UserWarning: \"sox\" backend is being deprecated. The default backend will be changed to \"sox_io\" backend in 0.8.0 and \"sox\" backend will be removed in 0.9.0. Please migrate to \"sox_io\" backend. Please refer to https://github.com/pytorch/audio/issues/903 for the detail.\n",
      "  '\"sox\" backend is being deprecated. '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/jsakuma/.cache/torch/hub/s3prl_cache/5e6b91abd59b390dc3f89225f0e7d26f5bcb6fac496a08110c8862e3b1bb93e7\n",
      "for https://dl.fbaipublicfiles.com/hubert/hubert_base_ls960.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 12:03:59 | INFO | fairseq.tasks.hubert_pretraining | current directory is /mnt/aoni04/jsakuma/development/timing-single/experiments/notebooks/Interspeech2022\n",
      "2022-07-24 12:03:59 | INFO | fairseq.tasks.hubert_pretraining | HubertPretrainingTask Config {'_name': 'hubert_pretraining', 'data': '/checkpoint/wnhsu/data/librispeech/960h/iter/250K_50hz_km100_mp0_65_v2', 'fine_tuning': False, 'labels': ['layer6.km500'], 'label_dir': None, 'label_rate': 50, 'sample_rate': 16000, 'normalize': False, 'enable_padding': False, 'max_keep_size': None, 'max_sample_size': 250000, 'min_sample_size': 32000, 'single_target': False, 'random_crop': True, 'pad_audio': False}\n",
      "2022-07-24 12:03:59 | INFO | fairseq.models.hubert.hubert | HubertModel Config: {'_name': 'hubert', 'label_rate': 50, 'extractor_mode': default, 'checkpoint_activations': False, 'encoder_layers': 12, 'encoder_embed_dim': 768, 'encoder_ffn_embed_dim': 3072, 'encoder_attention_heads': 12, 'activation_fn': gelu, 'dropout': 0.1, 'attention_dropout': 0.1, 'activation_dropout': 0.0, 'encoder_layerdrop': 0.05, 'dropout_input': 0.1, 'dropout_features': 0.1, 'final_dim': 256, 'untie_final_proj': False, 'layer_norm_first': False, 'conv_feature_layers': '[(512,10,5)] + [(512,3,2)] * 4 + [(512,2,2)] * 2', 'conv_bias': False, 'logit_temp': 0.1, 'target_glu': False, 'feature_grad_mult': 0.1, 'mask_length': 10, 'mask_prob': 0.8, 'mask_selection': static, 'mask_other': 0.0, 'no_mask_overlap': False, 'mask_min_space': 1, 'mask_channel_length': 10, 'mask_channel_prob': 0.0, 'mask_channel_selection': static, 'mask_channel_other': 0.0, 'no_mask_channel_overlap': False, 'mask_channel_min_space': 1, 'conv_pos': 128, 'conv_pos_groups': 16, 'latent_temp': [2.0, 0.5, 0.999995], 'skip_masked': False, 'skip_nomask': False}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import torch\n",
    "import pickle\n",
    "import string\n",
    "import random\n",
    "import librosa\n",
    "import torchaudio\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import nlpaug.flow as naf\n",
    "from collections import Counter\n",
    "import nlpaug.augmenter.audio as naa\n",
    "import nlpaug.augmenter.spectrogram as nas\n",
    "from torchvision.transforms import Normalize\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "sys.path.append('../../')\n",
    "#from src.systems.timing.timing_all_dasa_label_w_bert import System\n",
    "from src.systems.timing.w_label import System\n",
    "from src.utils.setup import process_config\n",
    "from src.datasets.timing_dataset import get_dataloader, get_dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"../../config/timing/da_sa_label_offset0_w_bert.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configuration: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:00<00:00, 276.79it/s]\n",
      "100%|██████████| 212/212 [00:00<00:00, 395.52it/s]\n"
     ]
    }
   ],
   "source": [
    "config = process_config(config_path, is_train=False)\n",
    "gpu_device = 1\n",
    "if gpu_device >= 0: config.gpu_device = gpu_device\n",
    "seed_everything(config.seed)\n",
    "ModelClass = globals()[config.system]\n",
    "\n",
    "device = torch.device(\"cuda:1\")\n",
    "\n",
    "#train_dataset = get_dataset(config, \"train\")\n",
    "val_dataset = get_dataset(config, \"val\")\n",
    "test_dataset = get_dataset(config, \"test\")\n",
    "#train_loader = get_dataloader(train_dataset, config, \"train\")\n",
    "val_loader = get_dataloader(val_dataset, config, \"val\")\n",
    "test_loader = get_dataloader(test_dataset, config, \"test\")\n",
    "\n",
    "#loader_dict = {\"train\": train_loader, \"val\": val_loader}\n",
    "#loader_dict = {\"train\": train_loader, \"val\": val_loader, \"test\": test_loader}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-24 12:04:08 | INFO | root | encoder self-attention layer type = self-attention\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2022-07-24 12:04:16 | INFO | root | encoder self-attention layer type = self-attention\n",
      "2022-07-24 12:04:16 | INFO | root | encoder self-attention layer type = self-attention\n"
     ]
    }
   ],
   "source": [
    "self = ModelClass(config, device, config.model_params.input_dim, val_dataset.num_class, val_dataset.dialog_acts_num_class, val_dataset.next_acts_num_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for System:\n\tsize mismatch for slu_model.dialog_acts_model.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for slu_model.dialog_acts_model.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for slu_model.dialog_acts_model.fc3.weight: copying a param with shape torch.Size([16, 64]) from checkpoint, the shape in current model is torch.Size([16, 128]).\n\tsize mismatch for slu_model.system_acts_model.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for slu_model.system_acts_model.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for slu_model.system_acts_model.fc3.weight: copying a param with shape torch.Size([17, 64]) from checkpoint, the shape in current model is torch.Size([17, 128]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-3e8874d36934>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#path = \"../../exp/timing/da_sa_label_offset0_w_bert/best_val_loss_model.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../../exp/timing/da_sa_label_offset0_w_bert0328/best_val_loss_model.pth\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1052\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1053\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for System:\n\tsize mismatch for slu_model.dialog_acts_model.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for slu_model.dialog_acts_model.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for slu_model.dialog_acts_model.fc3.weight: copying a param with shape torch.Size([16, 64]) from checkpoint, the shape in current model is torch.Size([16, 128]).\n\tsize mismatch for slu_model.system_acts_model.fc2.weight: copying a param with shape torch.Size([64, 256]) from checkpoint, the shape in current model is torch.Size([128, 256]).\n\tsize mismatch for slu_model.system_acts_model.fc2.bias: copying a param with shape torch.Size([64]) from checkpoint, the shape in current model is torch.Size([128]).\n\tsize mismatch for slu_model.system_acts_model.fc3.weight: copying a param with shape torch.Size([17, 64]) from checkpoint, the shape in current model is torch.Size([17, 128])."
     ]
    }
   ],
   "source": [
    "#path = \"../../exp/timing/da_sa_label_offset0_w_bert/best_val_loss_model.pth\"\n",
    "path = \"../../exp/timing/da_sa_label_offset0_w_bert0328/best_val_loss_model.pth\"\n",
    "self.load_state_dict(torch.load(path), strict=False)\n",
    "self.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A：システムが「発話せよ」と判断したタイミングの周囲で，実際にウィザードが発話している数\n",
    "# B：システムが「発話せよ」と判断したタイミングの外で，ウィーザードが発話した数\n",
    "# C：システムが「発話せよ」としたにも関わらず，ウィーザードはどこでも発話しなかった数（他の人が話し始めた，規定時間以上経過した　など）\n",
    "# D：システムは「発話せよ」と判断せず（＝発話するなと判断し），実際ウィザードも発話しなかった数\n",
    "# E：システムは「発話せよ」と判断しなかったにも関わらず，ウィザードがどこかで発話した数\n",
    "\n",
    "import math\n",
    "def timing_evaluation(y_pred, y_true, u_label, threshold=0.5, frame=50):\n",
    "    \n",
    "    target = False\n",
    "    pred = False\n",
    "    flag = True\n",
    "    fp_flag = False\n",
    "    AB, C, D, E = 0, 0, 0, 0    \n",
    "    pred_frame, target_frame = -1, -1\n",
    "    for i in range(1, len(y_pred)-1):\n",
    "                \n",
    "        #  予測が閾値を超えたタイミング\n",
    "        if y_pred[i] >= threshold and flag:\n",
    "            if u_label[i]>0.5:\n",
    "                fp_flag=True\n",
    "            else:\n",
    "                pred = True\n",
    "                flag = False\n",
    "                pred_frame = i\n",
    "\n",
    "        #  正解ラベルのタイミング\n",
    "        if y_true[i] > 0:\n",
    "            target = True\n",
    "            target_frame = i\n",
    "\n",
    "        \n",
    "    flag = True\n",
    "    if pred and target:\n",
    "        AB += 1\n",
    "    if (pred and not target) or fp_flag:\n",
    "        C += 1\n",
    "    if target and not pred:\n",
    "        E += 1\n",
    "    if not target and not pred:\n",
    "        D += 1\n",
    "\n",
    "    # TP, FP, FN, TN\n",
    "    return AB, C, E, D, pred_frame*frame, target_frame*frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 212/212 [01:39<00:00,  2.12it/s]\n"
     ]
    }
   ],
   "source": [
    "dic={\"TP\": 0, \"TP_label\": [], \"TP_pred\": [], \"FN\": 0, \"FN_label\": [], \"FP\": 0, \"TN\": 0}\n",
    "y_pred_list = []\n",
    "system_label_list = []\n",
    "y_label_list = []\n",
    "uttr_label_list = [] \n",
    "\n",
    "y_pred_list_test = []\n",
    "system_label_list_test = []\n",
    "y_label_list_test = []\n",
    "uttr_label_list_test = []\n",
    "da_label_list_test = []\n",
    "sa_label_list_test = []\n",
    "timing_loss = 0\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        uttr_nums = batch[0]\n",
    "        uttr_type = batch[1]\n",
    "        wavs = batch[2]\n",
    "        cnnae = batch[3].to(self.device)\n",
    "        fbank = batch[4].to(self.device)\n",
    "        input_lengths = batch[5]\n",
    "        timings = batch[6].to(self.device)\n",
    "        uttr_labels = batch[7].to(self.device)\n",
    "        labels = batch[8].to(self.device)\n",
    "        label_lengths = batch[9].to(self.device)\n",
    "        dialog_acts_labels = batch[10].to(self.device)\n",
    "        system_acts_labels = batch[11].to(self.device)\n",
    "        offset = batch[12]\n",
    "        duration = batch[13]\n",
    "        batch_size = len(wavs)\n",
    "        \n",
    "        outputs = self.slu_model.recog(batch)\n",
    "        text_encoding = outputs['bert_encoding']\n",
    "\n",
    "        i=0\n",
    "        subsampled = self.subsampling(fbank[i])\n",
    "        length = max(input_lengths[i])\n",
    "        embedding = torch.cat([subsampled[:, :length, :], cnnae[i][:, :length, :]], dim=-1)\n",
    "        del wavs\n",
    "        \n",
    "        #print(log_probs.shape, input_lengths.shape, labels.shape, label_lengths.shape)\n",
    "        if self.config.loss_params.asr_weight>0:\n",
    "            asr_loss = self.get_asr_loss(log_probs, input_lengths[i], labels, label_lengths)\n",
    "        else:\n",
    "            asr_loss = 0\n",
    "\n",
    "        out_list, label_list, uttr_list, da_list, sa_list, type_list = [], [], [], [], [], []\n",
    "        if self.config.loss_params.timing_weight>0:\n",
    "            assert batch_size==1, \"batch size must be set 1\"\n",
    "            for j in range(uttr_nums[i]):\n",
    "                if uttr_type[i][j] != 1:\n",
    "#                     if offset[0][i]>300:\n",
    "#                         continue\n",
    "\n",
    "                    dur = duration[i][j]\n",
    "\n",
    "                    if dur-self.T>0 and embedding[i].size(0)-(dur-self.T)>0:\n",
    "                        t = embedding[j].size(0)-(dur-self.T)\n",
    "                        tmp = torch.zeros([(dur-self.T), self.dialog_acts_num_class+self.system_acts_num_class+self.encoding_dim]).to(self.device)\n",
    "                        da = dialog_acts_labels[i][j].unsqueeze(0).repeat(t, 1)\n",
    "                        sa = system_acts_labels[i][j].unsqueeze(0).repeat(t, 1)\n",
    "                        text_emb = text_encoding[j].unsqueeze(0).repeat(t, 1)\n",
    "                        dasa = torch.cat([da, sa, text_emb], dim=-1)\n",
    "                        tmp2 = torch.cat([tmp, dasa], dim=0)\n",
    "                    else:\n",
    "                        tmp2 = torch.zeros([embedding[j].size(0), self.dialog_acts_num_class+self.system_acts_num_class]).to(self.device)\n",
    "                    emb = torch.cat([embedding[j], tmp2], dim=-1)\n",
    "                    emb = emb[:input_lengths[i][j]]\n",
    "\n",
    "                    t = emb.size(0)\n",
    "                    if uttr_type[i][j] == 0 or uttr_type[i][j] == 1:\n",
    "                        speaker = torch.zeros([t, 1]).to(self.device)\n",
    "                    else:\n",
    "                        speaker = torch.ones([t, 1]).to(self.device)\n",
    "\n",
    "                    emb = torch.cat([emb, speaker], dim=-1)\n",
    "\n",
    "                    output = self.timing_model(emb, uttr_labels[i][j][:input_lengths[i][j]])\n",
    "\n",
    "                    out_list.append(output[dur:])\n",
    "                    label_list.append(timings[i][j][:input_lengths[i][j]][dur:])#[-self.duration:])\n",
    "                    uttr_list.append(uttr_labels[i][j][:input_lengths[i][j]][dur:])#[-self.duration:])\n",
    "                    da_list.append(dialog_acts_labels[i][j])\n",
    "                    sa_list.append(system_acts_labels[i][j])\n",
    "        \n",
    "        for idx in range(len(out_list)):\n",
    "            y_pred = torch.sigmoid(out_list[idx]).detach().cpu().numpy()\n",
    "            system = label_list[idx].detach().cpu().numpy()\n",
    "            y_label = system[1:]-system[:-1]\n",
    "            uttr = uttr_list[idx].detach().cpu().numpy()\n",
    "            da = da_list[idx].detach().cpu().numpy()\n",
    "            sa = sa_list[idx].detach().cpu().numpy()\n",
    "            \n",
    "            y_pred_list+=list(y_pred)\n",
    "            system_label_list+=list(system)\n",
    "            y_label_list+=list(y_label)\n",
    "            uttr_label_list+=list(uttr)\n",
    "            \n",
    "            y_pred_list_test.append(y_pred)\n",
    "            system_label_list_test.append(system)\n",
    "            y_label_list_test.append(y_label)\n",
    "            uttr_label_list_test.append(uttr)\n",
    "            da_label_list_test.append(da)\n",
    "            sa_label_list_test.append(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1636/1636 [00:00<00:00, 6083.97it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "dic_test={\"TP\": 0, \"TP_label\": [], \"TP_pred\": [], \"FN\": 0, \"FN_label\": [], \"FP\": 0, \"TN\": 0,\n",
    "          \"DA_TP\": [], \"SA_TP\": [], \"DA_FN\": [], \"SA_FN\": [],\n",
    "         }\n",
    "thres = 0.5\n",
    "for i in tqdm(range(len(y_pred_list_test))):\n",
    "    TP, FP, FN, TN, pred, target = timing_evaluation(y_pred_list_test[i], y_label_list_test[i], uttr_label_list_test[i], threshold=thres)\n",
    "\n",
    "    if TP>0:\n",
    "        dic_test[\"TP\"]+=1\n",
    "        dic_test[\"TP_label\"].append(target)\n",
    "        dic_test[\"TP_pred\"].append(pred)\n",
    "    if FN>0:\n",
    "        dic_test[\"FN\"]+=1\n",
    "        dic_test[\"FN_label\"].append(target)\n",
    "    if FP>0: \n",
    "        dic_test[\"FP\"]+=FP\n",
    "    if TN>0:\n",
    "        dic_test[\"TN\"]+=TN\n",
    "      \n",
    "    if TP > 0 or FN >0:\n",
    "        da_labels = []\n",
    "        for idx, da in enumerate(da_label_list_test[i]):\n",
    "            if da==1:\n",
    "                #da_labels.append(da_labels_vocab[idx])\n",
    "                da_labels.append(idx)\n",
    "\n",
    "        sa_labels = []\n",
    "        for idx, sa in enumerate(sa_label_list_test[i]):            \n",
    "            if sa==1:\n",
    "                #sa_labels.append(sa_labels_vocab[idx])\n",
    "                sa_labels.append(idx)\n",
    "        if TP>0:\n",
    "            dic_test[\"DA_TP\"].append(da_labels)\n",
    "            dic_test[\"SA_TP\"].append(sa_labels)\n",
    "        else:\n",
    "            dic_test[\"DA_FN\"].append(da_labels)\n",
    "            dic_test[\"SA_FN\"].append(sa_labels)\n",
    "        \n",
    "        \n",
    "type_list = [1 for i in range(dic_test[\"TP\"])] + [0 for i in range(dic_test[\"FN\"])]\n",
    "df_test = pd.DataFrame({\n",
    "    'type': type_list, \n",
    "    'target': dic_test[\"TP_label\"]+dic_test[\"FN_label\"],\n",
    "#         'pred': dic[\"TP_pred\"]+[i+3000 for i in dic[\"FN_label\"]],\n",
    "    'pred': dic_test[\"TP_pred\"]+[1000000 for _ in range(dic_test[\"FN\"])],\n",
    "    'da': dic_test[\"DA_TP\"]+dic_test[\"DA_FN\"],\n",
    "    'sa': dic_test[\"SA_TP\"]+dic_test[\"SA_FN\"],\n",
    "})\n",
    "\n",
    "df_test['error'] = df_test['target'].values - df_test['pred'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "301 426 5 904 0\n",
      "許容誤差250ms - precision: 0.411, recall: 0.414, f1: 0.413, Acc: 0.737, , Acc2: 0.411\n",
      "\n",
      "491 236 5 904 0\n",
      "許容誤差500ms - precision: 0.671, recall: 0.675, f1: 0.673, Acc: 0.853, , Acc2: 0.671\n",
      "\n",
      "610 117 5 904 0\n",
      "許容誤差1000ms - precision: 0.833, recall: 0.839, f1: 0.836, Acc: 0.925, , Acc2: 0.833\n",
      "\n"
     ]
    }
   ],
   "source": [
    "err_list = [250, 500, 1000]\n",
    "for err in err_list:\n",
    "    \n",
    "    df = df_test\n",
    "    df_TP = df[df['type']==1]\n",
    "    \n",
    "    A = len(df_TP[abs(df_TP['error'])<=err])\n",
    "    B = len(df_TP[abs(df_TP['error'])>err])\n",
    "    C = dic_test['FP']\n",
    "    D = dic_test['TN']\n",
    "    E = dic_test['FN']\n",
    "    \n",
    "    recall = A / (A+B+E) if A+B+E>0 else 0\n",
    "    precision = A / (A+B+C) if (A+B+C)>0 else 0\n",
    "    f1 = 2 * recall * precision / (recall + precision) if (recall + precision)>0 else 0\n",
    "    \n",
    "    acc = (A+D) / (A+B+C+D+E)\n",
    "    acc2 = (A-E) / (A+B+C)\n",
    "    \n",
    "    print(A, B, C, D, E)\n",
    "    print(\"許容誤差{}ms - precision: {:.3f}, recall: {:.3f}, f1: {:.3f}, Acc: {:.3f}, , Acc2: {:.3f}\".format(err, precision, recall, f1, acc, acc2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
